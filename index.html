<!DOCTYPE html>

<html>

<head>
	<title>Xin Jiang</title>

	<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css" rel="stylesheet"
		integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">

	<script src="https://code.jquery.com/jquery-3.1.1.slim.min.js"
		integrity="sha384-A7FZj7v+d/sdmMqp/nOQwliLvUsJfDHW+k9Omg/a/EheAdgtzNs3hpfag6Ed950n"
		crossorigin="anonymous"></script>

	<style type="text/css">
		@import url("http://fonts.googleapis.com/css?family=Source+Sans+Pro:300,300italic,600,600italic");

		body {
			font-family: "Roboto", Helvetica, Arial, sans-serif;
			font-size: 16px;
			line-height: 1.5;
			font-weight: 300;
			background-color: #CDCDCD;
		}

		.content {
			width: 900px;
			padding: 25px 30px;
			margin: 25px auto;
			background-color: #fff;
			box-shadow: 0px 0px 10px #999;
			border-radius: 15px;
		}

		table {
			padding: 5px;
		}

		table.pub_table,
		td.pub_td1,
		td.pub_td2 {
			padding: 8px;
			width: 850px;
			border-collapse: separate;
			border-spacing: 15px;
			margin-top: -5px;
		}

		td.pub_td1 {
			width: 50px;
		}

		td.pub_td1 img {
			height: 120px;
			width: 160px;
		}

		div#container {
			margin-left: auto;
			margin-right: auto;
			width: 820px;
			text-align: left;
			position: relative;
			background-color: #FFF;
		}

		div#DocInfo {
			color: #1367a7;
			height: 158px;
		}

		h4,
		h3,
		h2,
		h1 {
			color: #3B3B3B;
		}

		h2 {
			font-size: 130%;
		}

		p {
			color: #5B5B5B;
			margin-bottom: 50px;
		}

		p.caption {
			color: #9B9B9B;
			text-align: left;
			width: 600px;
		}

		p.caption2 {
			color: #9B9B9B;
			text-align: left;
			width: 800px;
		}

		#header_img {
			position: absolute;
			top: 0px;
			right: 0px;
		}

		a:link,
		a:visited {
			color: #1367a7;
			text-decoration: none;
		}

		#mit_logo {
			position: absolute;
			left: 646px;
			top: 14px;
			width: 200px;
			height: 20px;
		}

		table.pub_table tr {
			outline: thin dotted #666666;
		}

		.papericon {
			border-radius: 8px;
			-moz-box-shadow: 3px 3px 6px #888;
			-webkit-box-shadow: 3px 3px 6px #888;
			box-shadow: 3px 3px 6px #888;
			width: 180px;
			margin-top: 5px;
			margin-left: 5px;
			margin-bottom: 5px;
		}

		.papericon_blank {
			width: 160px;
			margin-top: 5px;
			margin-left: 5px;
			margin-bottom: 5px;
		}

		.media {
			outline: thin dotted #666666;
			margin-bottom: 15px;
			margin-left: 10px;
		}

		.media-body {
			margin-top: 5px;
			padding-left: 20px;
		}

		.papers-selected h5,
		.papers-selected h4 {
			display: none;
		}

		.papers-selected .publication {
			display: none;
		}

		.paperhi-only {
			display: none;
		}

		.papers-selected .paperhi {
			display: flex;
		}

		.papers-selected .paperlo {
			display: none;
		}

		.awards-selected h5,
		.awards-selected h4 {
			display: none;
		}

		.awards-selected .publication {
			display: none;
		}

		.awardhi-only {
			display: none;
		}

		.awards-selected .awardhi {
			display: flex;
		}

		.awards-selected .awardlo {
			display: none;
		}

		.hidden>div {
			display: none;
		}

		.visible>div {
			display: block;
		}
	</style>

	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-23931362-2"></script>
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag () { dataLayer.push(arguments); }
		gtag('js', new Date());

		gtag('config', 'UA-23931362-2');
	</script>

	<script type="text/javascript">
		var myPix = new Array("img/wechat.png")
		function choosePic () {
			var randomNum = Math.floor(Math.random() * myPix.length);
			document.getElementById("myPicture").src = myPix[randomNum];
		};
	</script>

	<script>
		$(document).ready(function () {
			$('.paperlo button').click(function () {
				$('.papers-container').addClass('papers-selected');
			});
			$('.paperhi button').click(function () {
				$('.papers-container').removeClass('papers-selected');
			});

			$('.awardlo button').click(function () {
				$('.awards-container').addClass('awards-selected');
			});
			$('.awardhi button').click(function () {
				$('.awards-container').removeClass('awards-selected');
			});

			$('.text_container').addClass("hidden");

			$('.text_container').click(function () {
				var $this = $(this);

				if ($this.hasClass("hidden")) {
					$(this).removeClass("hidden").addClass("visible");
					$(this).removeClass("papericon");
				} else {
					$(this).removeClass("visible").addClass("hidden");
				}
			});


		});
	</script>

</head>


<body>
	<div class="content">
		<div id="container">

			<table>
				<tbody>
					<tr>
						<td><img id="myPicture" src="xxx" style="float:left; padding-right:20px" height="200px"></td>
						<script>choosePic();</script>
						<td>
							<div id="DocInfo">
								<h1>Xin Jiang (è’‹é‘«)</h1>
								Ph.D. Candidate<br>
								School of Computer Science and Engineering, Nanjing University of Science and Technology<br>
								Office: Room 2003, CSE Building<br>
								Email: xinjiang_at_njust.edu.cn<br>
								<!-- <a href="TH_CV.pdf" target="_blank" rel="external">CV</a> &bull; -->
								<a href="https://scholar.google.com.hk/citations?user=moDfIyoAAAAJ&hl=zh-CN" target="_blank"
									rel="external">Google Scholar</a> &bull; <a href="https://github.com/WhiteJiang" target="_blank"
									rel="external">Github</a><br>
							</div><br>
						</td>
					</tr>
				</tbody>
			</table>
			<br>

			<h2>About Me</h2>
			<!-- <p style="text-align:justify" ;> -->
			<p> Hi ğŸ˜„! I am a Ph.D. student at <a href="https://imag-njust.net/">Intelligent Media Analysis Group
					(IMAG)</a>,
				under the supervision of <a href="https://imag-njust.net/jinhui-tang/">Prof. Jinhui
					Tang</a>.
				My research interests lie in <span style="color: red; font-weight: bold;">Human-centered controlled and consistent generation</span>, e.g., <em>pose-guided image generation</em>, <em>dressing / try-on </em> and <em>face generation</em>.
				I was a passionate algorithm competitor, achieving over 50 top-three finishes, including five first-place wins in CCF A Workshops, and accumulating over 2 million RMB in post-tax prize money.
				I am fervently dedicated to <span
					style="text-decoration: underline; font-weight: bold; color: rgb(128, 0, 128);">SharingğŸŒ±, CollaboratingğŸ¤,
					AdvancingğŸš€, and InnovatingğŸ’¡</span> and  deeply passionate about academic research and fully committed to utilizing my research findings to address
				real-world challenges,
				thereby making meaningful and impactful contributions to society.

				<span style="color: red; font-weight: bold;"><em>If you are interested in collaboration or wish to get in touch
						with me, please feel free to reach out via email!
					</em></span> ğŸ˜Š
			</p>

			<h2><span style="color:red;font-size:27px"><strong>NEWS!</strong></span></h2>
			<ul style="height: 200px;overflow-y: auto">
				<div style="text-align: justify; display: block; margin-right: auto;">
					<li>2024/05: We released <a
							href="https://imagdressing.github.io/"><u>IMAGDressing-v1</u></a> for customizable virtual dressing.</li>

					<li>2024/04: One paper was accepted by IEEE TKDE.</li>
					<li>2024/02: One paper was accepted by IEEE TSCVT.</li>
					<li>2023/12: One paper was accepted by AAAI 2023.</li>
				</div>
			</ul>

			<h2>Research Experience</h2>

			<div>
				&emsp; <strong>Nanjing University of Science and Technology, Nanjing China (Sep 2022 - Present)</strong>
				<!-- <a href="https://www.sutd.edu.sg/" target="_blank" rel="external"> -->
				<img border="0" src="img/imag.png" align="right" width="120" height="100" />
				<!-- </a> -->
				<ul>
					<li>
						PhD student, <a href="https://imag-njust.net/" target="_blank" rel="external">IMAG</a>
					</li>
					<li>
						Hosted by Prof. <a href="" target="_blank"
							rel="external">Zechao
							Li</a>,
					</li>
				</ul>
			</div>




			<!-- <h2>Research Interests</h2>

			<ul>
				<h6><strong style="color:#ff0000">Learning From Limited or Imperfect Data</strong></h6>
				<li> Multimedia: Zero/Few-shot Learning, Fine-Grained Visual Classification/Retrieval, ... </li>
				<li> Computer Vision: Multi-Modality Object Localization/Detection/Segmentation </li>
			</ul> -->

			<div class="papers-container papers-selected">
				<h5 class="paperlo">All Publications<button type="button" class="ml-3 btn btn-light">Show selected</button></h5>
				<h5 class="paperhi paperhi-only">Selected Publications<button type="button" class="ml-3 btn btn-light">Show
						all</button></h5>

				<h5 class="pt-2 pb-1">2024</h5>
				<div class="publication media">
					<div class="media-body">
						V-Express: Conditional Dropout for Progressive Training of Portrait Video Generation</strong></a><br>
						Cong Wang, Kuan Tian, Jun Zhang, Yonghang Guan, Feng Luo, Fei Shen, Zhiwei Jiangâ€ , Qing Gu, Xiao Han, Wei Yang <br>
						Under Review
						[<a href=https://arxiv.org/abs/2406.02511" target="_blank" rel="external"><strong
								style="color:darkblue"></strong>PDF</a>]
						[<a href="https://github.com/tencent-ailab/V-Express/">Code</a>]<br>
					</div>
				</div>

				<div class="publication media">
					<div class="media-body">
						Ensembling Diffusion Models via Adaptive Feature Aggregation</strong></a><br>
						Cong Wang, Kuan Tian, Yonghang Guan, Jun Zhang, Zhiwei Jiangâ€ , Fei Shen, Xiao Han, Qing Gu, Wei Yang <br>
						Under Review
						[<a href=https://arxiv.org/abs/2406.02511" target="_blank" rel="external"><strong
								style="color:darkblue"></strong>PDF</a>]
						[<a href="https://github.com/tencent-ailab/V-Express/">Code</a>]<br>
					</div>
				</div>


				<h5 class="pt-2 pb-1">2023</h5>

				<div class="publication media paperhi">
					<div class="media-body">
						<strong>Advancing Pose-Guided Image Synthesis with Progressive Conditional Diffusion Models</strong><br>
						Fei Shen, Hu Ye, Jun Zhang, Cong Wang, Xiao Han, Wei Yang <br>
						ICLR 2024 [<a href="https://arxiv.org/pdf/2310.06313.pdf" target="_blank" rel="external"><strong
								style="color:darkblue"></strong>PDF</a>]
								[<a href="https://github.com/tencent-ailab/PCDMs">Code</a>]<br>
					</div>
				</div>
				
				<div class="publication media paperhi">
					<div class="media-body">
						<strong>Triplet Contrastive Representation Learning for Unsupervised Vehicle Re-identification</strong><br>
						Fei Shen, Xiaoyu Du, Liyan Zhang, Xiangbo Shu, and Jinhui Tang <br>
						Under Review 
						[<a href="https://arxiv.org/pdf/2301.09498.pdf" target="_blank" rel="external"><strong
								style="color:darkblue"></strong>PDF</a>]
								[<a href="https://github.com/muzishen/TCRL">Code</a>]<br>
					</div>
				</div>

				<div class="publication media paperhi">
					<div class="media-body">
						<strong>Pedestrian-specific Bipartite-aware Similarity Learning for Text-based Person Retrieval</strong><br>
						Fei Shen, Xiangbo Shu, Xiaoyu Du, and Jinhui Tang <br>
						ACM Multmedia 2023 
						[<a href="https://dl.acm.org/doi/abs/10.1145/3581783.3612009" target="_blank" rel="external"><strong
								style="color:darkblue"></strong>PDF</a>]
								[<a href="https://github.com/muzishen/PBSL">Code</a>]<br>
					</div>
				</div>



				<div class="publication media paperhi">
					<div class="media-body">
						GiT: Graph interactive transformer for vehicle re-identification</strong></a><br>
						Fei Shen, Yi Xie, Jianqing Zhu, Xiaobin Zhu, and Huanqiang Zeng <br>
						IEEE Transactions on Image Processing
						[<a href="https://ieeexplore.ieee.org/document/10026500" target="_blank" rel="external"><strong
								style="color:darkblue"></strong>PDF</a>]
						[<a href="https://github.com/muzishen/ReID-GiT">Code</a>]
						<br>
					</div>
				</div>

				<h5 class="pt-2 pb-1">2022</h5>
				<div class="publication media">
					<div class="media-body">
						HSGM: A hierarchical similarity graph module for object re-identification</strong></a><br>
						Fei Shen, Xiaoxiao Peng, Lisheng Wang, Xingmeng Hao, Mei Shu, and Yajun Wang <br>
						IEEE ICME 2022 
						[<a href="https://ieeexplore.ieee.org/document/9859883" target="_blank" rel="external"><strong
								style="color:darkblue"></strong>PDF</a>]
						[<a href="https://github.com/muzishen/HSGM">Code</a>]<br>
					</div>
				</div>

				<div class="publication media">
					<div class="media-body">
						Enhancing part features via contrastive attention module for vehicle re-identification</strong></a><br>
						Manyu Li, Mengwan Wei, Xin He, and Fei Shen* <br>
						IEEE ICIP 2022 
						[<a href="https://ieeexplore.ieee.org/document/9897943" target="_blank" rel="external"><strong
								style="color:darkblue"></strong>PDF</a>]
						[<a href="https://github.com/muzishen/HPGN">Code</a>]<br>
					</div>
				</div>

				<div class="publication media">
					<div class="media-body">
						A Novel Multi-Frequency Coordinated Module for SAR Ship Detection</strong></a><br>
						Chenchen Qiao, Fei Shen, Xuejun Wang, Ruixin Wang, Fang Cao, Sixian Zhao, and Chang Li <br>
						IEEE ICTAI 2022
						[<a href="https://ieeexplore.ieee.org/document/10097976" target="_blank" rel="external"><strong
								style="color:darkblue"></strong>PDF</a>]
						[<a href="https://github.com/muzishen/HPGN">Code</a>]<br>
					</div>
				</div>

				<div class="publication media">
					<div class="media-body">
						A sample-proxy dual triplet loss function for object re-identification</strong></a><br>
						Hanxiao Wu, Fei Shen, Jianqing Zhu, Huanqiang Zeng, Xiaobin Zhu, and Zhen Lei <br>
						IET Image Processing
						[<a href="https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/ipr2.12593" target="_blank"
							rel="external"><strong style="color:darkblue"></strong>PDF</a>]
						[<a href="https://github.com/muzishen/HPGN">Code</a>]<br>
					</div>
				</div>

				<h5 class="pt-2 pb-1">2021</h5>
				<div class="publication media paperhi">
					<div class="media-body">
						Exploring spatial significance via hybrid pyramidal graph network for vehicle
						re-identification</strong></a><br>
						Fei Shen, Jianqing Zhu, Xiaobin Zhu, Yi Xie, and Jingchang Huang <br>
						IEEE Transactions on Intelligent Transportation Systems
						[<a href="https://ieeexplore.ieee.org/document/9457192" target="_blank" rel="external"><strong
								style="color:darkblue"></strong>PDF</a>]
						[<a href="https://github.com/muzishen/HPGN">Code</a>]<br>
					</div>
				</div>

				<div class="publication media">
					<div class="media-body">
						An efficient multiresolution network for vehicle reidentification</strong></a><br>
						Fei Shen, Jianqing Zhu, Xiaobin Zhu, Jingchang Huang, Huanqiang Zeng, Zhen Lei, and Canhui Cai <br>
						IEEE Internet of Things Journal
						[<a href="https://ieeexplore.ieee.org/document/9569744" target="_blank" rel="external"><strong
								style="color:darkblue"></strong>PDF</a>]
						[<a href="https://github.com/muzishen/EMRN">Code</a>]<br>
					</div>
				</div>

				<div class="publication media">
					<div class="media-body">
						Object Re-identification Using Teacher-Like and Light Students</strong></a><br>
						Yi Xie, Hanxiao Wu, Fei Shen, Jianqing Zhu, and Huanqiang Zeng <br>
						BMVC 2021
						[<a href="https://www.bmvc2021-virtualconference.com/assets/papers/1193.pdf" target="_blank"
							rel="external"><strong style="color:darkblue"></strong>PDF</a>]
						[<a href="https://github.com/muzishen/EMRN">Code</a>]<br>
					</div>
				</div>

				<div class="publication media">
					<div class="media-body">
						Viewpoint robust knowledge distillation for accelerating vehicle re-identification</a><br>
						Yi Xie, Fei Shen, Jianqing Zhu, and Huanqiang Zeng <br>
						EURASIP Journal on Advances in Signal Processing
						[<a href="https://link.springer.com/article/10.1186/s13634-021-00767-x" target="_blank"
							rel="external"><strong style="color:darkblue"></strong>PDF</a>]
						[<a href="https://github.com/muzishen/EMRN">Code</a>]<br>
					</div>
				</div>
			</div>



			<!--<sup>&#x2709</sup>-->
			<div class="awards-container awards-selected">
				<h5 class="awardlo">All Awards<button type="button" class="ml-3 btn btn-light">Show selected</button>
				</h5>
				<h5 class="awardhi awardhi-only">Selected Awards<button type="button" class="ml-3 btn btn-light">Show
						all</button></h5>

				<h5 class="pt-2 pb-1">2022 </h5>


				<div class="publication media">
					<div class="media-body">
						2022åä¸ºdigixå…¨çƒæ ¡å›­ç²¾è‹±æŒ‘æˆ˜èµ›-è½¦é“çº¿æ¸²æŸ“æ£€æµ‹,<a href="img/2022_digix_2nd.pdf">äºšå†›</a><br>
						æ²ˆé£, è’‹é‘«, é™ˆå˜‰ç…œ [<a href="https://github.com/muzishen/Pet-ReID-IMAG">Code</a>] <br>
						ä¸»åŠå•ä½ï¼šæ±Ÿè‹çœäººå·¥æ™ºèƒ½å­¦ä¼š, åä¸ºæŠ€æœ¯æœ‰é™å…¬å¸ <br>
					</div>
				</div>


				<div class="publication media">
					<div class="media-body">
						2022é¦–å±Šâ€œå…´æ™ºæ¯â€å…¨å›½äººå·¥æ™ºèƒ½åˆ›æ–°åº”ç”¨å¤§èµ›,<a href="img/2022å…´æ™ºæ¯ä¸‰ç­‰å¥–.pdf">ä¸‰ç­‰å¥–</a><br>
						æ²ˆé£, è’‹é‘«, ç‹æ™‹, ä½•æ¨å‡Œ<br>
						ä¸»åŠå•ä½ï¼šå…¨å›½äººå·¥æ™ºèƒ½å¤§ä¼š <br>
					</div>
				</div>


			</div>




			<h2>Honors</h2>
			<div>
				<ul>
					<li>First Prize Scholarship of Nanjing University of Science and Technology, 2022, 2023</li>
					<li>åä¸ºå¥–å­¦é‡‘, å—äº¬ç†å·¥å¤§å­¦, 2021</li>
					<!-- <li>First-class Innovation Scholarship, Ministry of Industry and Information Technology of China, 2017</li> -->
				</ul>
			</div>

			<h2>Professional Services</h2>
			<div>
				<ul>
					<li>
						<b>Journal Reviewer</b>: </br>
						<!-- &emsp; â€¢ IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI) </br> -->
						&emsp; â€¢ TNNLS| T-CSVT</br>
						<!-- &emsp; â€¢ IEEE Transactions on Intelligent Transportation Systems (T-ITS) </br>
						&emsp; â€¢ IEEE Transactions on Vehicular Technology (T-VT) </br>
						&emsp; â€¢ IEEE Transactions on Neural Networks and Learning Systems (T-NNLS) </br>
						&emsp; â€¢ IEEE Internet of Things Journal (IOTJ) </br>
						&emsp; â€¢ Neural Computing and Applications (NCA) </br>
						&emsp; â€¢ Neurocomputing (NC) </br></br> -->
					</li>
					<li>
						<b>Conference Reviewer / Program Committee Member</b>: </br>
						&emsp; â€¢ ACM MM| ICME </br>
						<!-- &emsp; &emsp; â€¢ European Conference on Computer Vision 2024 </br>
						&emsp; â€¢ International Conference on Multimedia and Expo 2022-2024 </br>
						&emsp; â€¢ ACM International Conference on Multimedia 2022,2024 </br></br> -->
						<!-- &emsp; â€¢ IEEE International Conference on Multimedia Information Processing and Retrieval (MIPR): 2021-2023</br> -->
					</li>
				</ul>
			</div>

			<!--<h2>Links</h2>
			
			<ul>
				<li><a href="https://dongzhang89.github.io/" target="_blank" rel="external">Dong Zhang (å¼ å†¬)</a>,
					<a href="https://ruiyan1995.github.io/" target="_blank" rel="external">Rui Yan (ä¸¥é”)</a>,
					<a href="https://cser-tang-hao.github.io/" target="_blank" rel="external">Hao Tang (å”æ˜Š)</a>,
					<a href="https://tenvence.github.io/" target="_blank" rel="external">Cong Wang (ç‹èª)</a>.
					<!-- <a href="https://zhengye1995.github.io/" target="_blank" rel="external">Ye Zheng (éƒ‘çƒ¨)</a>,
				<a href="https://github.com/whut2962575697" target="_blank" rel="external">Xin He (ä½•æ–°)</a>,
				<a href="https://github.com/CarryHJR" target="_blank" rel="external">Jiarui Hu (èƒ¡ä½³ç¿)</a>, -->
				</li>
				
				<!--			<li>-->
				<!--				NJUST: <a href="https://ruiyan1995.github.io/" target="_blank" rel="external">Rui Yan</a>, <a href="https://dongzhang89.github.io/" target="_blank" rel="external">Dong Zhang</a>-->
				<!--			</li>-->
				<!--			<li>-->
				<!--				NKU: Zhimao Peng-->
				<!--			</li>-->
				<!--			<li>-->
				<!--				DUT: <a href="https://wdhudiekou.github.io/" target="_blank" rel="external">Di Wang</a>-->
				<!--			</li>-->
				<!-- <i style="color:darkcyan">
				I'm always interested in meeting new people and hearing about potential collaborations. If you'd like to work
				together or get in contact with me, please email me.
			</i> -->
			</ul>

		</div>

</body>

</html>
